# -*- coding: utf-8 -*-
"""Prediction whether healthy or not.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cU1EOR2lWKfNCwjDT1sCLXjAK018pkoP
"""

import pandas  as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.neighbors import KNeighborsClassifier
import seaborn as sns
import torch

# from google.colab import drive
# drive.mount('/content/drive')

data = pd.read_csv("/content/dataframe (1).csv")

print(data.info())

data=data.replace('M',0)
data=data.replace('m',0)
data=data.replace('F',1)
data=data.replace('Healthy',1)
data=data.replace('Patients',0)
data=data.replace('No',0)
data=data.replace('YES',1)
data=data.replace('Left',0)
data=data.replace('Right',1)
data=data.replace('O',0)
data=data.replace('A',0)

df = data.replace(r'^\s*$', np.nan, regex=True)

# df = pd.DataFrame(np.squeeze(X), columns=['ID', 'Age', 'Gender', 'Handedness', 'History', 'P1', 'P2', 'P3', 'P4',
#        'P5', 'P6', 'P7', 'P8', 'P9', 'PT', 'G1', 'G2', 'G3', 'G4', 'G5', 'G6',
#        'G7', 'GT', 'W1', 'W2', 'W3', 'W4', 'W5', 'W6', 'W7', 'WT', 'H1', 'H2',
#        'H3', 'H4', 'H5', 'H6', 'H7', 'H8', 'H9', 'H10', 'H11', 'H12', 'H13',
#        'H14', 'H15', 'H16', 'H17', 'HT', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6',
#        'M7', 'M8', 'M9', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16',
#        'M17', 'M18', 'M19', 'M20', 'M21', 'M22', 'M23', 'M24', 'M25', 'M26',
#        'M27', 'M28', 'M29', 'M30', 'MT'])

# normalise the amount column
# data['normAmount'] = StandardScaler().fit_transform(np.array(data['Amount']).reshape(-1, 1))

# drop Time and Amount columns as they are not relevant for prediction purpose
# data = data.drop(['Date-Visit1'], axis = 1)

# as you can see there are 492 fraud transactions.
df['Patient'].value_counts()

import itertools
import math

# columns = data.columns
columns = ['P1', 'P2', 'P3', 'P4','P5', 'P6', 'P7', 'P8', 'P9', 'PT', 'G1', 'G2', 'G3', 'G4', 'G5', 'G6',
       'G7', 'GT', 'W1', 'W2', 'W3', 'W4', 'W5', 'W6', 'W7', 'WT', 'H1', 'H2',
       'H3', 'H4', 'H5', 'H6', 'H7', 'H8', 'H9', 'H10', 'H11', 'H12', 'H13',
       'H14', 'H15', 'H16', 'H17', 'HT', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6',
       'M7', 'M8', 'M9', 'M10', 'M11', 'M12', 'M13', 'M14', 'M15', 'M16',
       'M17', 'M18', 'M19', 'M20', 'M21', 'M22', 'M23', 'M24', 'M25', 'M26',
       'M27', 'M28', 'M29', 'M30', 'MT']

# Generate all possible combinations of two features
combinations = itertools.combinations(columns, 2)

# Iterate through the list of combinations
for combination in combinations:
  # Get the names of the two features in this combination
  V1, V2 = combination
  # Create a new feature to store the result of the multiplication
  new_feature_name = f'{V1}_{V2}'
  df[new_feature_name] = 0
  # data[new_feature_name] = data.[v1 * data.v2
  # Iterate through each row in the DataFrame
  for index, row in df.iterrows():
    # Multiply the values of v1 and v2 for this row
    result = (row[V1]) + (row[V2])
     # Store the result in the corresponding row of the new feature
    # result = result.tolist
    df.at[index, new_feature_name] = (result)

  #   data[index,]
# data = data.copy()

"""1. sum of test scores
2. product of test score sums with personal details
"""

import itertools
import math

# columns = data.columns
columns = ['PT', 'GT', 'WT', 'HT', 'MT']

# Generate all possible combinations of two features
combinations = itertools.combinations(columns, 2)
# Iterate through the list of combinations
for combination in combinations:
  # Get the names of the two features in this combination
  V1, V2 = combination
  # Create a new feature to store the result of the multiplication
  new_feature_name = f'{V1}_{V2}'
  df[new_feature_name] = 0
  # data[new_feature_name] = data.[v1 * data.v2
  # Iterate through each row in the DataFrame
  for index, row in df.iterrows():
    # Multiply the values of v1 and v2 for this row
    result = (row[V1]) + (row[V2])
     # Store the result in the corresponding row of the new feature
    # result = result.tolist
    df.at[index, new_feature_name] = (result)


combinations = itertools.combinations(columns, 3)
# Iterate through the list of combinations
for combination in combinations:
  # Get the names of the two features in this combination
  V1, V2, V3 = combination
  # Create a new feature to store the result of the multiplication
  new_feature_name = f'{V1}_{V2}_{V3}'
  df[new_feature_name] = 0
  # data[new_feature_name] = data.[v1 * data.v2
  # Iterate through each row in the DataFrame
  for index, row in df.iterrows():
    # Multiply the values of v1 and v2 for this row
    result = (row[V1]) + (row[V2]) + (row[V3])
     # Store the result in the corresponding row of the new feature
    # result = result.tolist
    df.at[index, new_feature_name] = (result)

combinations = itertools.combinations(columns, 4)

# Iterate through the list of combinations
for combination in combinations:
  # Get the names of the two features in this combination
  V1, V2, V3, V4 = combination
  # Create a new feature to store the result of the multiplication
  new_feature_name = f'{V1}_{V2}_{V3}_{V4}'
  df[new_feature_name] = 0
  # data[new_feature_name] = data.[v1 * data.v2
  # Iterate through each row in the DataFrame
  for index, row in df.iterrows():
    # Multiply the values of v1 and v2 for this row
    result = (row[V1]) + (row[V2]) + (row[V3]) + (row[V4])
     # Store the result in the corresponding row of the new feature
    # result = result.tolist
    df.at[index, new_feature_name] = (result)


combinations = itertools.combinations(columns, 5)

# Iterate through the list of combinations
for combination in combinations:
  # Get the names of the two features in this combination
  V1, V2, V3, V4, V5 = combination
  # Create a new feature to store the result of the multiplication
  new_feature_name = f'{V1}_{V2}_{V3}_{V4}_{V5}'
  df[new_feature_name] = 0
  # data[new_feature_name] = data.[v1 * data.v2
  # Iterate through each row in the DataFrame
  for index, row in df.iterrows():
    # Multiply the values of v1 and v2 for this row
    result = (row[V1]) + (row[V2]) + (row[V3]) + (row[V4]) + row[V5]
     # Store the result in the corresponding row of the new feature
    # result = result.tolist
    df.at[index, new_feature_name] = (result)


  #   data[index,]
# data = data.copy()

import itertools
import math


columns = ['PT', 'GT', 'WT', 'HT', 'MT']
column = 'Gender'

# Generate a list of combinations of one feature and the gender column
combinations = [(col, column) for col in columns]

# Iterate through the list of combinations
for combination in combinations:
    # Get the names of the feature and gender column
    v1, v2 = combination
    # Create a new feature to store the result of the multiplication
    new_feature_name = f'{v1}_{v2}'
    df[new_feature_name] = df[v1] * df[v2]

    for index, row in df.iterrows():
        # Multiply the values of v1 and v2 for this row
        result = row[v1] * row[v2]
        # Store the result in the corresponding row of the new feature
        df.at[index, new_feature_name] = result


columns = ['PT', 'GT', 'WT', 'HT', 'MT']
column = 'History'

# Generate a list of combinations of one feature and the gender column
combinations = [(col, column) for col in columns]

# Iterate through the list of combinations
for combination in combinations:
    # Get the names of the feature and gender column
    v1, v2 = combination
    # Create a new feature to store the result of the multiplication
    new_feature_name = f'{v1}_{v2}'
    df[new_feature_name] = df[v1] * df[v2]

    for index, row in df.iterrows():
        # Multiply the values of v1 and v2 for this row
        result = row[v1] * row[v2]
        # Store the result in the corresponding row of the new feature
        df.at[index, new_feature_name] = result


columns = ['PT', 'GT', 'WT', 'HT', 'MT']
column = 'Handedness'

# Generate a list of combinations of one feature and the gender column
combinations = [(col, column) for col in columns]

# Iterate through the list of combinations
for combination in combinations:
    # Get the names of the feature and gender column
    v1, v2 = combination
    # Create a new feature to store the result of the multiplication
    new_feature_name = f'{v1}_{v2}'
    df[new_feature_name] = df[v1] * df[v2]

    for index, row in df.iterrows():
        # Multiply the values of v1 and v2 for this row
        result = row[v1] * row[v2]
        # Store the result in the corresponding row of the new feature
        df.at[index, new_feature_name] = result

df

from scipy.stats import chi2_contingency

# Load your dataset into a pandas DataFrame


# Get the list of column names in the DataFrame

columns = df.columns
# columns = ['Age','Gender','Handedness','History','PHQ','GAD','HDRS']

# Iterate through the list of columns
for column in columns:
  # Select the feature for which you want to calculate the chi-square statistic
  feature = df[column]

  # Create a contingency table for the feature
  contingency_table = pd.crosstab(feature, df["Patient"])

  # Calculate the chi-square statistic for the contingency table
  chi2, p, dof, expected = chi2_contingency(contingency_table)

  # Print the name of the feature and the chi-square statistic
  if chi2>=60:

    print(f'Feature: {column}')
    print(f'Chi-square statistic: {chi2:.3f}')
    print(f'p-value: {p:.3f}')

  else:

    df= df.drop(f'{column}',axis=1)

print(df.columns)

from sklearn.model_selection import train_test_split

X = df.drop("Patient", axis=1)
y = df["Patient"]

# split into 70:30 ration
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)

y_train=y_train.ravel()
# describes info about train and test set
print("Number transactions X_train dataset: ", X_train.shape)
print("Number transactions y_train dataset: ", y_train.shape)
print("Number transactions X_test dataset: ", X_test.shape)
print("Number transactions y_test dataset: ", y_test.shape)

y_train = pd.DataFrame(y_train, columns = ['Patient'])

# np.any(np.isnan(X_train))
# np.all(np.isfinite(X_train))
# np.any(np.isnan(y_train))
# np.all(np.isfinite(y_train))
X_train = X_train.reset_index(drop=True)
#y_train = y_train.reset_index()

# X_test.dropna(axis=0)

X_test.fillna(value=X_test.mean(), inplace=True)
y_test.fillna(0.00, inplace=True)
X_train.fillna(value=X_test.mean(), inplace=True)

from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state = 2)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))
print('After OverSampling, the shape of train_y: {} \n'.format(y_train_res.shape))



y_train_res['Patient'].value_counts()

def add_gaussian_noise(data, mean=0, std=1):
    noise = np.random.normal(mean, std, data.shape)
    return data + noise

Xtrain_new = add_gaussian_noise(X_train_res)

"""Applying Linear Regression"""

# logistic regression object
lr = LogisticRegression()

# train the model on train set
lr.fit(Xtrain_new, y_train_res)

y_pred = lr.predict(X_test)

# print classification report
print(classification_report(y_test, y_pred))

fpr, tpr, thresholds = roc_curve(y_test, y_pred)

roc_auc = auc(  fpr,tpr)
roc_auc

"""Applying Random Forest"""

from sklearn.ensemble import RandomForestClassifier
# creating a RF classifier
clf = RandomForestClassifier(n_estimators = 100)

# Training the model on the training dataset
# fit function is used to train the model using the training sets as parameters
clf.fit(Xtrain_new, y_train_res)

# performing predictions on the test dataset
y_pred = clf.predict(X_test)

# metrics are used to find accuracy or error
from sklearn import metrics
print()

# using metrics module for accuracy calculation
print("ACCURACY OF THE MODEL: ", metrics.accuracy_score(y_test, y_pred))

fpr, tpr, thresholds = roc_curve(y_test, y_pred)

roc_auc = auc(  fpr,tpr)
roc_auc

"""Applying Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier()

# Train Decision Tree Classifer
clf = clf.fit(X_train,y_train)

#Predict the response for test dataset
y_pred = clf.predict(X_test)

fpr, tpr, thresholds = roc_curve(y_test, y_pred)

roc_auc = auc(  fpr,tpr)
roc_auc

"""Applying Ridge Regression"""

from sklearn.linear_model import Ridge

ridge_reg = Ridge(alpha=50, max_iter=100, tol=0.1)

ridge_reg.fit(Xtrain_new, y_train_res)

y_pred = ridge_reg.predict(X_test)

from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report
fpr, tpr, thresholds = roc_curve(y_test, y_pred)

roc_auc = auc(  fpr,tpr)
roc_auc

# check if model is overfitted by accuracy cal on training data

y_pred = clf.predict(Xtrain_new)

fpr, tpr, thresholds = roc_curve(y_train_res, y_pred)

roc_auc = auc(  fpr,tpr)
roc_auc